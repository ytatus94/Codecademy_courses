{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 2 Day 3 & 4\n",
    "## Lambda Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "102\n",
      "0\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "Got an A!\n",
      "Did not get an A...\n",
      "Did not get an A...\n"
     ]
    }
   ],
   "source": [
    "add_two = lambda my_input: my_input + 2\n",
    "print(add_two(3))\n",
    "print(add_two(100))\n",
    "print(add_two(-2))\n",
    "\n",
    "is_substring = lambda my_string: my_string in \"This is the master string\"\n",
    "print(is_substring('I'))\n",
    "print(is_substring('am'))\n",
    "print(is_substring('the'))\n",
    "print(is_substring('master'))\n",
    "\n",
    "check_if_A_grade = lambda grade: 'Got an A!' if grade >= 90 else 'Did not get an A...'\n",
    "print(check_if_A_grade(91))\n",
    "print(check_if_A_grade(70))\n",
    "print(check_if_A_grade(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating, Loading, and Manipulating Data with Pandas\n",
    "Firstly, import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DataFrame\n",
    "* Use dictionary\n",
    "  * The order is not the same as defined in the DataFrame.\n",
    "* Use list\n",
    "  * Use `columns=[name1, name2, ...]` to define column name.\n",
    "  * The order is the same as defined in DataFrame.\n",
    "* Read from CSV file\n",
    "  * Use `pd.read_csv()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         name         address  age\n",
      "0  John Smith    123 Main St.   34\n",
      "1    Jane Doe  456 Maple Ave.   28\n",
      "2   Joe Schmo    789 Broadway   51\n",
      "   Product ID Product Name  Color\n",
      "0           1      t-shirt   blue\n",
      "1           2      t-shirt  green\n",
      "2           3        skirt    red\n",
      "3           4        skirt  black\n",
      "         name         address  age\n",
      "0  John Smith    123 Main St.   34\n",
      "1    Jane Doe  456 Maple Ave.   28\n",
      "2   Joe Schmo    789 Broadway   51\n",
      "   Store ID       Location  Number of Employees\n",
      "0         1      San Diego                  100\n",
      "1         2    Los Angeles                  120\n",
      "2         3  San Francisco                   90\n",
      "3         4     Sacramento                  115\n",
      "            City  Population  Median Age\n",
      "0      Maplewood      100000          40\n",
      "1          Wayne      350000          33\n",
      "2  Forrest Hills      300000          35\n",
      "3        Paramus      400000          55\n",
      "4     Hackensack      290000          39\n"
     ]
    }
   ],
   "source": [
    "# Use dictionary\n",
    "df1 = pd.DataFrame({\n",
    "    'name': ['John Smith', 'Jane Doe', 'Joe Schmo'],\n",
    "    'address': ['123 Main St.', '456 Maple Ave.', '789 Broadway'],\n",
    "    'age': [34, 28, 51]\n",
    "})\n",
    "\n",
    "print(df1)\n",
    "\n",
    "# Use dictionary\n",
    "df1 = pd.DataFrame({\n",
    "  'Product ID': [1, 2, 3, 4],\n",
    "  # add Product Name and Color here\n",
    "  'Product Name': ['t-shirt', 't-shirt', 'skirt', 'skirt'],\n",
    "  'Color':['blue', 'green', 'red', 'black']\n",
    "})\n",
    "\n",
    "print(df1)\n",
    "\n",
    "# Use list\n",
    "df2 = pd.DataFrame([\n",
    "    ['John Smith', '123 Main St.', 34],\n",
    "    ['Jane Doe', '456 Maple Ave.', 28],\n",
    "    ['Joe Schmo', '789 Broadway', 51]\n",
    "    ],\n",
    "    columns=['name', 'address', 'age'])\n",
    "print(df2)\n",
    "\n",
    "# Use list\n",
    "df2 = pd.DataFrame([\n",
    "  [1, 'San Diego', 100],\n",
    "  [2, 'Los Angeles', 120],\n",
    "  # Fill in rows 3 and 4\n",
    "  [3, 'San Francisco', 90],\n",
    "  [4, 'Sacramento', 115]\n",
    "],\n",
    "  columns=[\n",
    "    #add column names here\n",
    "    'Store ID', 'Location', 'Number of Employees'\n",
    "  ])\n",
    "\n",
    "print(df2)\n",
    "\n",
    "# Read from CSV file\n",
    "df = pd.read_csv('unit2_sample.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect a DataFrame\n",
    "* The method `df.head()` gives the first 5 rows of a DataFrame.\n",
    "* The method `df.info()` gives some statistics for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                       name   genre  year  imdb_rating\n",
      "0   1                                     Avatar  action  2009          7.9\n",
      "1   2                             Jurassic World  action  2015          7.3\n",
      "2   3                               The Avengers  action  2012          8.1\n",
      "3   4                            The Dark Knight  action  2008          9.0\n",
      "4   5  Star Wars: Episode I - The Phantom Menace  action  1999          6.6\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 220 entries, 0 to 219\n",
      "Data columns (total 5 columns):\n",
      "id             220 non-null int64\n",
      "name           220 non-null object\n",
      "genre          220 non-null object\n",
      "year           220 non-null int64\n",
      "imdb_rating    220 non-null float64\n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 8.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('unit2_imdb.csv')\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select column\n",
    "* Use `df.[name]` to select a column\n",
    "* Use `df[ [name1, name2, ...] ]` to select multiple columns\n",
    "\n",
    "### Select rows\n",
    "* Use `df.iloc[index]` to select a row.\n",
    "* Uze `df.iloc[start:end]` to select multiple rows.\n",
    "\n",
    "### Select Rows with Logic\n",
    "* Use `==`, `>`, `<`, `!=`, `|`, `&`, `.isin()`\n",
    "\n",
    "### Setting indices\n",
    "* Use `reset_index()`\n",
    "* Use `drop=True` to drop old index.\n",
    "* Use `inplace=True` to modify existing DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      month  clinic_east  clinic_north  clinic_south  clinic_west\n",
      "0   January          100           100            23          100\n",
      "1  February           51            45           145           45\n",
      "2     March           81            96            65           96\n",
      "3     April           80            80            54          180\n",
      "4       May           51            54            54          154\n",
      "5      June          112           109            79          129\n",
      "<class 'pandas.core.series.Series'>\n",
      "0    100\n",
      "1     45\n",
      "2     96\n",
      "3     80\n",
      "4     54\n",
      "5    109\n",
      "Name: clinic_north, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "   clinic_north  clinic_south\n",
      "0           100            23\n",
      "1            45           145\n",
      "2            96            65\n",
      "3            80            54\n",
      "4            54            54\n",
      "5           109            79\n",
      "month           March\n",
      "clinic_east        81\n",
      "clinic_north       96\n",
      "clinic_south       65\n",
      "clinic_west        96\n",
      "Name: 2, dtype: object\n",
      "   month  clinic_east  clinic_north  clinic_south  clinic_west\n",
      "3  April           80            80            54          180\n",
      "4    May           51            54            54          154\n",
      "5   June          112           109            79          129\n",
      "     month  clinic_east  clinic_north  clinic_south  clinic_west\n",
      "0  January          100           100            23          100\n",
      "   month  clinic_east  clinic_north  clinic_south  clinic_west\n",
      "2  March           81            96            65           96\n",
      "3  April           80            80            54          180\n",
      "      month  clinic_east  clinic_north  clinic_south  clinic_west\n",
      "0   January          100           100            23          100\n",
      "1  February           51            45           145           45\n",
      "2     March           81            96            65           96\n",
      "      month  clinic_east  clinic_north  clinic_south  clinic_west\n",
      "1  February           51            45           145           45\n",
      "3     April           80            80            54          180\n",
      "5      June          112           109            79          129\n",
      "      month  clinic_east  clinic_north  clinic_south  clinic_west\n",
      "1  February           51            45           145           45\n",
      "3     April           80            80            54          180\n",
      "5      June          112           109            79          129\n",
      "   index     month  clinic_east  clinic_north  clinic_south  clinic_west\n",
      "0      1  February           51            45           145           45\n",
      "1      3     April           80            80            54          180\n",
      "2      5      June          112           109            79          129\n",
      "      month  clinic_east  clinic_north  clinic_south  clinic_west\n",
      "0  February           51            45           145           45\n",
      "1     April           80            80            54          180\n",
      "2      June          112           109            79          129\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame([\n",
    "  ['January', 100, 100, 23, 100],\n",
    "  ['February', 51, 45, 145, 45],\n",
    "  ['March', 81, 96, 65, 96],\n",
    "  ['April', 80, 80, 54, 180],\n",
    "  ['May', 51, 54, 54, 154],\n",
    "  ['June', 112, 109, 79, 129]],\n",
    "  columns=['month', 'clinic_east',\n",
    "           'clinic_north', 'clinic_south',\n",
    "           'clinic_west']\n",
    ")\n",
    "print(df)\n",
    "\n",
    "# Select Columns\n",
    "clinic_north = df['clinic_north']\n",
    "print(type(clinic_north)) # not a DataFrame\n",
    "print(clinic_north)\n",
    "print(type(df))\n",
    "\n",
    "# Selecting Multiple Columns\n",
    "clinic_north_south = df[['clinic_north', 'clinic_south']]\n",
    "print(type(clinic_north_south)) # is a DataFrame\n",
    "print(clinic_north_south)\n",
    "\n",
    "# Select Rows\n",
    "march = df.iloc[2]\n",
    "print(march)\n",
    "\n",
    "# Selecting Multiple Rows\n",
    "april_may_june = df.iloc[3:]\n",
    "print(april_may_june)\n",
    "\n",
    "# Select Rows with Logic\n",
    "january = df[df.month == 'January']\n",
    "print(january)\n",
    "\n",
    "march_april = df[(df.month == 'March') | (df.month == 'April')]\n",
    "print(march_april)\n",
    "\n",
    "january_february_march = df[df.month.isin(['January', 'February', 'March'])]\n",
    "print(january_february_march)\n",
    "\n",
    "# Setting indices\n",
    "df2 = df.loc[[1, 3, 5]]\n",
    "print(df2)\n",
    "\n",
    "df3 = df2.reset_index()\n",
    "print(df2)\n",
    "print(df3) # contains old and new indies\n",
    "\n",
    "df3 = df2.reset_index(inplace=True, drop=True)\n",
    "print(df2) # Modify the existing DataFrame\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id first_name last_name  gender                         email  \\\n",
      "0  54791    Rebecca   Lindsay  female  RebeccaLindsay57@hotmail.com   \n",
      "1  53450      Emily     Joyce  female        EmilyJoyce25@gmail.com   \n",
      "2  91987      Joyce    Waller  female        Joyce.Waller@gmail.com   \n",
      "3  14437     Justin  Erickson    male   Justin.Erickson@outlook.com   \n",
      "4  79357     Andrew     Banks    male              AB4318@gmail.com   \n",
      "\n",
      "      shoe_type shoe_material shoe_color  \n",
      "0         clogs  faux-leather      black  \n",
      "1  ballet flats  faux-leather       navy  \n",
      "2       sandles        fabric      black  \n",
      "3         clogs  faux-leather        red  \n",
      "4         boots       leather      brown  \n",
      "0     RebeccaLindsay57@hotmail.com\n",
      "1           EmilyJoyce25@gmail.com\n",
      "2           Joyce.Waller@gmail.com\n",
      "3      Justin.Erickson@outlook.com\n",
      "4                 AB4318@gmail.com\n",
      "5           JulieMarsh59@gmail.com\n",
      "6                 TJ5470@gmail.com\n",
      "7           Janice.Hicks@gmail.com\n",
      "8        GabrielPorter24@gmail.com\n",
      "9        FrancesPalmer50@gmail.com\n",
      "10         JessicaHale25@gmail.com\n",
      "11      LawrenceParker44@gmail.com\n",
      "12         SusanDennis58@gmail.com\n",
      "13                DO2680@gmail.com\n",
      "14       Rebecca.Charles@gmail.com\n",
      "15              JC2072@hotmail.com\n",
      "16              VS4753@outlook.com\n",
      "17          RoyTillman20@gmail.com\n",
      "18       Thomas.Roberson@gmail.com\n",
      "19         ANewton1977@outlook.com\n",
      "Name: email, dtype: object\n",
      "      id first_name last_name  gender                      email shoe_type  \\\n",
      "9  62083    Frances    Palmer  female  FrancesPalmer50@gmail.com    wedges   \n",
      "\n",
      "  shoe_material shoe_color  \n",
      "9       leather      white  \n",
      "       id first_name   last_name  gender                         email  \\\n",
      "0   54791    Rebecca     Lindsay  female  RebeccaLindsay57@hotmail.com   \n",
      "1   53450      Emily       Joyce  female        EmilyJoyce25@gmail.com   \n",
      "3   14437     Justin    Erickson    male   Justin.Erickson@outlook.com   \n",
      "4   79357     Andrew       Banks    male              AB4318@gmail.com   \n",
      "6   20487     Thomas      Jensen    male              TJ5470@gmail.com   \n",
      "7   76971     Janice       Hicks  female        Janice.Hicks@gmail.com   \n",
      "8   21586    Gabriel      Porter    male     GabrielPorter24@gmail.com   \n",
      "10  91629    Jessica        Hale  female       JessicaHale25@gmail.com   \n",
      "12  45832      Susan      Dennis  female       SusanDennis58@gmail.com   \n",
      "14  73431    Rebecca     Charles  female     Rebecca.Charles@gmail.com   \n",
      "16  39888    Vincent  Stephenson    male            VS4753@outlook.com   \n",
      "17  35961        Roy     Tillman    male        RoyTillman20@gmail.com   \n",
      "\n",
      "       shoe_type shoe_material shoe_color  \n",
      "0          clogs  faux-leather      black  \n",
      "1   ballet flats  faux-leather       navy  \n",
      "3          clogs  faux-leather        red  \n",
      "4          boots       leather      brown  \n",
      "6          clogs        fabric       navy  \n",
      "7          clogs  faux-leather       navy  \n",
      "8          clogs       leather      brown  \n",
      "10         clogs       leather        red  \n",
      "12  ballet flats        fabric      white  \n",
      "14         boots  faux-leather      white  \n",
      "16         boots       leather      black  \n",
      "17         boots       leather      white  \n"
     ]
    }
   ],
   "source": [
    "#Part 1: reading the csv\n",
    "orders = pd.read_csv('unit2_shoefly.csv')\n",
    "\n",
    "#Part 2: inspecting the first five lines of data\n",
    "print(orders.head(5))\n",
    "\n",
    "#Part 3: selecting the column 'email'\n",
    "emails = orders.email\n",
    "print(emails)\n",
    "\n",
    "#Part 4: the Frances Palmer incident\n",
    "frances_palmer = orders[(orders.first_name == 'Frances') & (orders.last_name == 'Palmer')]\n",
    "print(frances_palmer)\n",
    "\n",
    "#Part 5: Comfy feet means more time on the street\n",
    "comfy_shoes = orders[orders.shoe_type.isin(['clogs', 'boots', 'ballet flats'])]\n",
    "print(comfy_shoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 2 Day 5\n",
    "## Modifying DataFrames in Pandas\n",
    "### Add a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Product ID   Description  Cost to Manufacture  Price\n",
      "0           1  3 inch screw                  0.5   0.75\n",
      "1           2   2 inch nail                  0.1   0.25\n",
      "2           3        hammer                  3.0   5.50\n",
      "3           4   screwdriver                  2.5   3.00\n",
      "   Product ID   Description  Cost to Manufacture  Price Sold in Bulk?\n",
      "0           1  3 inch screw                  0.5   0.75           Yes\n",
      "1           2   2 inch nail                  0.1   0.25           Yes\n",
      "2           3        hammer                  3.0   5.50            No\n",
      "3           4   screwdriver                  2.5   3.00            No\n",
      "   Product ID   Description  Cost to Manufacture  Price Sold in Bulk?  \\\n",
      "0           1  3 inch screw                  0.5   0.75           Yes   \n",
      "1           2   2 inch nail                  0.1   0.25           Yes   \n",
      "2           3        hammer                  3.0   5.50            No   \n",
      "3           4   screwdriver                  2.5   3.00            No   \n",
      "\n",
      "  Is taxed?  \n",
      "0       Yes  \n",
      "1       Yes  \n",
      "2       Yes  \n",
      "3       Yes  \n",
      "   Product ID   Description  Cost to Manufacture  Price Sold in Bulk?  \\\n",
      "0           1  3 inch screw                  0.5   0.75           Yes   \n",
      "1           2   2 inch nail                  0.1   0.25           Yes   \n",
      "2           3        hammer                  3.0   5.50            No   \n",
      "3           4   screwdriver                  2.5   3.00            No   \n",
      "\n",
      "  Is taxed?  Revenue  \n",
      "0       Yes     0.25  \n",
      "1       Yes     0.15  \n",
      "2       Yes     2.50  \n",
      "3       Yes     0.50  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame([\n",
    "  [1, '3 inch screw', 0.5, 0.75],\n",
    "  [2, '2 inch nail', 0.10, 0.25],\n",
    "  [3, 'hammer', 3.00, 5.50],\n",
    "  [4, 'screwdriver', 2.50, 3.00]\n",
    "],\n",
    "  columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price']\n",
    ")\n",
    "print(df)\n",
    "\n",
    "# Add columns here\n",
    "df['Sold in Bulk?'] = ['Yes', 'Yes', \"No\", 'No']\n",
    "print(df)\n",
    "\n",
    "df['Is taxed?'] = 'Yes'\n",
    "print(df)\n",
    "\n",
    "df['Revenue'] = df.Price - df['Cost to Manufacture']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Operations\n",
    "* Use `apply()` to apply on all values in a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Name                 Email\n",
      "0  JOHN SMITH  john.smith@gmail.com\n",
      "1    Jane Doe        jdoe@yahoo.com\n",
      "2   joe schmo  joeschmo@hotmail.com\n",
      "         Name                 Email Lowercase Name\n",
      "0  JOHN SMITH  john.smith@gmail.com     john smith\n",
      "1    Jane Doe        jdoe@yahoo.com       jane doe\n",
      "2   joe schmo  joeschmo@hotmail.com      joe schmo\n",
      "         Name                 Email Lowercase Name Email Provider\n",
      "0  JOHN SMITH  john.smith@gmail.com     john smith      gmail.com\n",
      "1    Jane Doe        jdoe@yahoo.com       jane doe      yahoo.com\n",
      "2   joe schmo  joeschmo@hotmail.com      joe schmo    hotmail.com\n"
     ]
    }
   ],
   "source": [
    "#from string import lower\n",
    "\n",
    "df = pd.DataFrame([\n",
    "  ['JOHN SMITH', 'john.smith@gmail.com'],\n",
    "  ['Jane Doe', 'jdoe@yahoo.com'],\n",
    "  ['joe schmo', 'joeschmo@hotmail.com']\n",
    "],\n",
    "columns=['Name', 'Email'])\n",
    "print(df)\n",
    "\n",
    "# Add columns here\n",
    "#df['Lowercase Name'] = df.Name.apply(lower) # This line works in Codecademy but not works in jupyter notebook\n",
    "df['Lowercase Name'] = df.Name.str.lower() # This works in jupyter notebook\n",
    "print(df)\n",
    "\n",
    "df['Email Provider'] = df.Email.apply(\n",
    "    lambda x: x.split('@')[-1]\n",
    "    )\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lambda function\n",
    "* If Statements:\n",
    "  `lambda x: [OUTCOME IF TRUE] \\\n",
    "    if [CONDITIONAL] \\\n",
    "    else [OUTCOME IF FALSE]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "oh hi mark!\n",
      "Tg\n"
     ]
    }
   ],
   "source": [
    "mylambda = lambda x: (x * 2) + 3\n",
    "print(mylambda(5))\n",
    "\n",
    "stringlambda = lambda x: x.lower()\n",
    "print(stringlambda(\"Oh Hi Mark!\"))\n",
    "\n",
    "mylambda = lambda x: x[0] + x[len(x)-1]\n",
    "print(mylambda('This is a string'))\n",
    "\n",
    "# If Statements\n",
    "myfunction = lambda x: 40 + (x - 40) * 1.50 \\\n",
    "    if x > 40 else x\n",
    "    \n",
    "mylambda = lambda x: 'Welcome to BattleCity!' if x >= 13 else 'You must be over 13'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying a Lambda to a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id               name  hourly_wage  hours_worked  last_name\n",
      "0   10310      Lauren Durham           19            43     Durham\n",
      "1   18656      Grace Sellers           17            40    Sellers\n",
      "2   61254  Shirley Rasmussen           16            30  Rasmussen\n",
      "3   16886        Brian Rojas           18            47      Rojas\n",
      "4   89010    Samantha Mosley           11            38     Mosley\n",
      "5   87246       Louis Guzman           14            39     Guzman\n",
      "6   20578     Denise Mcclure           15            40    Mcclure\n",
      "7   12869      James Raymond           15            32    Raymond\n",
      "8   53461       Noah Collier           18            35    Collier\n",
      "9   14746    Donna Frederick           20            41  Frederick\n",
      "10  71127       Shirley Beck           14            32       Beck\n",
      "11  92522    Christina Kelly            8            44      Kelly\n",
      "12  22447        Brian Noble           11            39      Noble\n",
      "13  61654          Randy Key           16            38        Key\n",
      "14  16988      Diana Stewart           14            48    Stewart\n",
      "15  68619       Timothy Sosa           14            42       Sosa\n",
      "16  59949      Betty Skinner           11            48    Skinner\n",
      "17  81418      Janet Maxwell           12            38    Maxwell\n",
      "18  27267   Madison Johnston           20            37   Johnston\n",
      "19  19985   Virginia Nichols           13            49    Nichols\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('unit2_employees.csv')\n",
    "\n",
    "# Add columns here\n",
    "get_last_name = lambda x: x.split()[-1]\n",
    "df['last_name'] = df.name.apply(get_last_name)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying a Lambda to a Row\n",
    "* Use `row.column_name` or `row[‘column_name’]` to access particular values of the row.\n",
    "* Use argument `axis=1` in `apply()` to indicate this is a row operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id               name  hourly_wage  hours_worked  last_name  \\\n",
      "0   10310      Lauren Durham           19            43     Durham   \n",
      "1   18656      Grace Sellers           17            40    Sellers   \n",
      "2   61254  Shirley Rasmussen           16            30  Rasmussen   \n",
      "3   16886        Brian Rojas           18            47      Rojas   \n",
      "4   89010    Samantha Mosley           11            38     Mosley   \n",
      "5   87246       Louis Guzman           14            39     Guzman   \n",
      "6   20578     Denise Mcclure           15            40    Mcclure   \n",
      "7   12869      James Raymond           15            32    Raymond   \n",
      "8   53461       Noah Collier           18            35    Collier   \n",
      "9   14746    Donna Frederick           20            41  Frederick   \n",
      "10  71127       Shirley Beck           14            32       Beck   \n",
      "11  92522    Christina Kelly            8            44      Kelly   \n",
      "12  22447        Brian Noble           11            39      Noble   \n",
      "13  61654          Randy Key           16            38        Key   \n",
      "14  16988      Diana Stewart           14            48    Stewart   \n",
      "15  68619       Timothy Sosa           14            42       Sosa   \n",
      "16  59949      Betty Skinner           11            48    Skinner   \n",
      "17  81418      Janet Maxwell           12            38    Maxwell   \n",
      "18  27267   Madison Johnston           20            37   Johnston   \n",
      "19  19985   Virginia Nichols           13            49    Nichols   \n",
      "\n",
      "    total_earned  \n",
      "0          845.5  \n",
      "1          680.0  \n",
      "2          480.0  \n",
      "3          909.0  \n",
      "4          418.0  \n",
      "5          546.0  \n",
      "6          600.0  \n",
      "7          480.0  \n",
      "8          630.0  \n",
      "9          830.0  \n",
      "10         448.0  \n",
      "11         368.0  \n",
      "12         429.0  \n",
      "13         608.0  \n",
      "14         728.0  \n",
      "15         602.0  \n",
      "16         572.0  \n",
      "17         456.0  \n",
      "18         740.0  \n",
      "19         695.5  \n"
     ]
    }
   ],
   "source": [
    "total_earned = lambda row: (row.hourly_wage * 40) + ((row.hourly_wage * 1.5) * (row.hours_worked - 40)) \\\n",
    "\tif row.hours_worked > 40 \\\n",
    "  else row.hourly_wage * row.hours_worked\n",
    "  \n",
    "df['total_earned'] = df.apply(total_earned, axis = 1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename column\n",
    "* Use `df.columns = ['name1', 'name2', ...]`\n",
    "* Use `df.rename(columns={'old_name1' : 'new_name1', 'old_name2':'new_name2', ...}, inplace=True)`.\n",
    "  * The argument `inplace=True` means to edit the original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   name  age\n",
      "0  John   23\n",
      "1  Jane   29\n",
      "2   Sue   21\n",
      "3  Fred   18\n",
      "  First Name  Age\n",
      "0       John   23\n",
      "1       Jane   29\n",
      "2        Sue   21\n",
      "3       Fred   18\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'name': ['John', 'Jane', 'Sue', 'Fred'],\n",
    "    'age': [23, 29, 21, 18]\n",
    "})\n",
    "print(df)\n",
    "\n",
    "df.columns = ['First Name', 'Age']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                       name   genre  year  imdb_rating\n",
      "0   1                                     Avatar  action  2009          7.9\n",
      "1   2                             Jurassic World  action  2015          7.3\n",
      "2   3                               The Avengers  action  2012          8.1\n",
      "3   4                            The Dark Knight  action  2008          9.0\n",
      "4   5  Star Wars: Episode I - The Phantom Menace  action  1999          6.6\n",
      "   ID                                      Title Category  Year Released  \\\n",
      "0   1                                     Avatar   action           2009   \n",
      "1   2                             Jurassic World   action           2015   \n",
      "2   3                               The Avengers   action           2012   \n",
      "3   4                            The Dark Knight   action           2008   \n",
      "4   5  Star Wars: Episode I - The Phantom Menace   action           1999   \n",
      "\n",
      "   Rating  \n",
      "0     7.9  \n",
      "1     7.3  \n",
      "2     8.1  \n",
      "3     9.0  \n",
      "4     6.6  \n",
      "   id                                movie_title   genre  year  imdb_rating\n",
      "0   1                                     Avatar  action  2009          7.9\n",
      "1   2                             Jurassic World  action  2015          7.3\n",
      "2   3                               The Avengers  action  2012          8.1\n",
      "3   4                            The Dark Knight  action  2008          9.0\n",
      "4   5  Star Wars: Episode I - The Phantom Menace  action  1999          6.6\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('unit2_imdb.csv')\n",
    "print(df.head())\n",
    "\n",
    "# Rename columns here\n",
    "df.columns = ['ID', 'Title', 'Category', 'Year Released', 'Rating']\n",
    "#print(df)\n",
    "print(df.head())\n",
    "\n",
    "df = pd.read_csv('unit2_imdb.csv')\n",
    "\n",
    "# Rename columns here\n",
    "df.rename(columns={'name':'movie_title'},inplace=True)\n",
    "#print(df)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id first_name last_name  gender                         email  \\\n",
      "0  54791    Rebecca   Lindsay  female  RebeccaLindsay57@hotmail.com   \n",
      "1  53450      Emily     Joyce  female        EmilyJoyce25@gmail.com   \n",
      "2  91987      Joyce    Waller  female        Joyce.Waller@gmail.com   \n",
      "3  14437     Justin  Erickson    male   Justin.Erickson@outlook.com   \n",
      "4  79357     Andrew     Banks    male              AB4318@gmail.com   \n",
      "\n",
      "      shoe_type shoe_material shoe_color  \n",
      "0         clogs  faux-leather      black  \n",
      "1  ballet flats  faux-leather       navy  \n",
      "2       sandles        fabric      black  \n",
      "3         clogs  faux-leather        red  \n",
      "4         boots       leather      brown  \n",
      "      id first_name last_name  gender                         email  \\\n",
      "0  54791    Rebecca   Lindsay  female  RebeccaLindsay57@hotmail.com   \n",
      "1  53450      Emily     Joyce  female        EmilyJoyce25@gmail.com   \n",
      "2  91987      Joyce    Waller  female        Joyce.Waller@gmail.com   \n",
      "3  14437     Justin  Erickson    male   Justin.Erickson@outlook.com   \n",
      "4  79357     Andrew     Banks    male              AB4318@gmail.com   \n",
      "\n",
      "      shoe_type shoe_material shoe_color shoe_source         salutation  \n",
      "0         clogs  faux-leather      black       vegan   Dear Ms. Lindsay  \n",
      "1  ballet flats  faux-leather       navy       vegan     Dear Ms. Joyce  \n",
      "2       sandles        fabric      black       vegan    Dear Ms. Waller  \n",
      "3         clogs  faux-leather        red       vegan  Dear Mr. Erickson  \n",
      "4         boots       leather      brown      animal     Dear Mr. Banks  \n"
     ]
    }
   ],
   "source": [
    "orders = pd.read_csv('unit2_shoefly.csv')\n",
    "\n",
    "print(orders.head(5))\n",
    "\n",
    "orders['shoe_source'] = orders.shoe_material.apply(lambda x: 'animal' if x == 'leather'else 'vegan')\n",
    "\n",
    "orders['salutation'] = orders.apply(lambda row: \\\n",
    "'Dear Mr. ' + row['last_name']\\\n",
    "if row['gender'] == 'male'\\\n",
    "else 'Dear Ms. ' + row['last_name'],\\\n",
    "axis=1)\n",
    "\n",
    "#print(orders)\n",
    "print(orders.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 2 Day 6 & 7\n",
    "## Project: Petal Power\n",
    "### Petal Power Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        location  product_type         product_description  quantity  price\n",
      "0  Staten Island         seeds                       daisy         4   6.99\n",
      "1  Staten Island         seeds                  calla lily        46  19.99\n",
      "2  Staten Island         seeds                      tomato        85  13.99\n",
      "3  Staten Island  garden tools                        rake         4  13.99\n",
      "4  Staten Island  garden tools                 wheelbarrow         0  89.99\n",
      "5  Staten Island  garden tools                       spade        93  19.99\n",
      "6  Staten Island  pest_control               insect killer        74  12.99\n",
      "7  Staten Island  pest_control                 weed killer         8  23.99\n",
      "8  Staten Island       planter  20 inch terracotta planter         0  17.99\n",
      "9  Staten Island       planter      8 inch plastic planter        53   3.99\n",
      "        location  product_type         product_description  quantity  price\n",
      "0  Staten Island         seeds                       daisy         4   6.99\n",
      "1  Staten Island         seeds                  calla lily        46  19.99\n",
      "2  Staten Island         seeds                      tomato        85  13.99\n",
      "3  Staten Island  garden tools                        rake         4  13.99\n",
      "4  Staten Island  garden tools                 wheelbarrow         0  89.99\n",
      "5  Staten Island  garden tools                       spade        93  19.99\n",
      "6  Staten Island  pest_control               insect killer        74  12.99\n",
      "7  Staten Island  pest_control                 weed killer         8  23.99\n",
      "8  Staten Island       planter  20 inch terracotta planter         0  17.99\n",
      "9  Staten Island       planter      8 inch plastic planter        53   3.99\n",
      "0                         daisy\n",
      "1                    calla lily\n",
      "2                        tomato\n",
      "3                          rake\n",
      "4                   wheelbarrow\n",
      "5                         spade\n",
      "6                 insect killer\n",
      "7                   weed killer\n",
      "8    20 inch terracotta planter\n",
      "9        8 inch plastic planter\n",
      "Name: product_description, dtype: object\n",
      "    location product_type product_description  quantity  price\n",
      "10  Brooklyn        seeds               daisy        50   6.99\n",
      "11  Brooklyn        seeds          calla lily         0  19.99\n",
      "12  Brooklyn        seeds              tomato         0  13.99\n",
      "         location  product_type         product_description  quantity  price  \\\n",
      "0   Staten Island         seeds                       daisy         4   6.99   \n",
      "1   Staten Island         seeds                  calla lily        46  19.99   \n",
      "2   Staten Island         seeds                      tomato        85  13.99   \n",
      "3   Staten Island  garden tools                        rake         4  13.99   \n",
      "4   Staten Island  garden tools                 wheelbarrow         0  89.99   \n",
      "5   Staten Island  garden tools                       spade        93  19.99   \n",
      "6   Staten Island  pest_control               insect killer        74  12.99   \n",
      "7   Staten Island  pest_control                 weed killer         8  23.99   \n",
      "8   Staten Island       planter  20 inch terracotta planter         0  17.99   \n",
      "9   Staten Island       planter      8 inch plastic planter        53   3.99   \n",
      "10       Brooklyn         seeds                       daisy        50   6.99   \n",
      "11       Brooklyn         seeds                  calla lily         0  19.99   \n",
      "12       Brooklyn         seeds                      tomato         0  13.99   \n",
      "13       Brooklyn  garden tools                        rake        15  13.99   \n",
      "14       Brooklyn  garden tools                 wheelbarrow        82  89.99   \n",
      "15       Brooklyn  garden tools                       spade        36  19.99   \n",
      "16       Brooklyn  pest_control               insect killer        80  12.99   \n",
      "17       Brooklyn  pest_control                 weed killer        76  23.99   \n",
      "18       Brooklyn       planter  20 inch terracotta planter         5  17.99   \n",
      "19       Brooklyn       planter      8 inch plastic planter        26   3.99   \n",
      "20         Queens         seeds                       daisy        57   6.99   \n",
      "21         Queens         seeds                  calla lily        95  19.99   \n",
      "22         Queens         seeds                      tomato        45  13.99   \n",
      "23         Queens  garden tools                        rake        21  13.99   \n",
      "24         Queens  garden tools                 wheelbarrow        98  89.99   \n",
      "25         Queens  garden tools                       spade        26  19.99   \n",
      "26         Queens  pest_control               insect killer         0  12.99   \n",
      "27         Queens  pest_control                 weed killer        16  23.99   \n",
      "28         Queens       planter  20 inch terracotta planter        87  17.99   \n",
      "\n",
      "    in_stock  \n",
      "0       True  \n",
      "1       True  \n",
      "2       True  \n",
      "3       True  \n",
      "4      False  \n",
      "5       True  \n",
      "6       True  \n",
      "7       True  \n",
      "8      False  \n",
      "9       True  \n",
      "10      True  \n",
      "11     False  \n",
      "12     False  \n",
      "13      True  \n",
      "14      True  \n",
      "15      True  \n",
      "16      True  \n",
      "17      True  \n",
      "18      True  \n",
      "19      True  \n",
      "20      True  \n",
      "21      True  \n",
      "22      True  \n",
      "23      True  \n",
      "24      True  \n",
      "25      True  \n",
      "26     False  \n",
      "27      True  \n",
      "28      True  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 1\n",
    "inventory = pd.read_csv('unit2_inventory.csv')\n",
    "#print(inventory)\n",
    "# 2\n",
    "print(inventory.head(10))\n",
    "# 3\n",
    "staten_island = inventory.iloc[0:10] # select first 10 rows\n",
    "print(staten_island)\n",
    "# 4\n",
    "product_request = staten_island['product_description']\n",
    "print(product_request)\n",
    "# 5\n",
    "seed_request = inventory[(inventory['location'] == 'Brooklyn') & (inventory['product_type'] == 'seeds')]\n",
    "print(seed_request)\n",
    "# 6\n",
    "inventory['in_stock'] = inventory.quantity.apply(lambda quantity: True if quantity > 0 else False)\n",
    "print(inventory)\n",
    "# 7\n",
    "total_value = inventory.price * inventory.quantity\n",
    "#print(total_value)\n",
    "# 8\n",
    "combine_lambda = lambda row: \\\n",
    "    '{} - {}'.format(row.product_type,\n",
    "                     row.product_description)\n",
    "# 9\n",
    "inventory['full_description'] = inventory.apply(combine_lambda, axis=1)\n",
    "#print(inventory.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 2 Day 8\n",
    "## Aggregates in Pandas\n",
    "* Use `df.column_name.command()`\n",
    "\n",
    "|Command|Description|\n",
    "|:--|:--|\n",
    "|mean|Average of all values in column|\n",
    "|std|Standard deviation|\n",
    "|median|Median|\n",
    "|max|Maximum value in column|\n",
    "|min|Minimum value in column|\n",
    "|count|Number of values in column|\n",
    "|nunique|Number of unique values in column|\n",
    "|unique|List of unique values in column|\n",
    "* Use `df.groupby('column1').column2.measurement()`\n",
    "* Use `.reset_index()` to get a DataFrame\n",
    "  * `df.groupby('column1').column2.measurement().reset_index()`\n",
    "* Use `apply()` and lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id first_name    last_name                         email     shoe_type  \\\n",
      "0  41874       Kyle         Peck          KylePeck71@gmail.com  ballet flats   \n",
      "1  31349  Elizabeth    Velazquez      EVelazquez1971@gmail.com         boots   \n",
      "2  43416      Keith     Saunders              KS4047@gmail.com       sandles   \n",
      "3  56054       Ryan      Sweeney     RyanSweeney14@outlook.com       sandles   \n",
      "4  77402      Donna  Blankenship              DB3807@gmail.com     stilettos   \n",
      "5  97148     Albert       Dillon       Albert.Dillon@gmail.com        wedges   \n",
      "6  19998     Judith       Hewitt      JudithHewitt98@gmail.com     stilettos   \n",
      "7  83290      Kayla       Hardin        Kayla.Hardin@gmail.com     stilettos   \n",
      "8  77867     Steven  Blankenship  Steven.Blankenship@gmail.com        wedges   \n",
      "9  54885      Carol   Mclaughlin              CM3415@gmail.com  ballet flats   \n",
      "\n",
      "  shoe_material shoe_color  price  \n",
      "0  faux-leather      black  385.0  \n",
      "1        fabric      brown  388.0  \n",
      "2       leather       navy  346.0  \n",
      "3        fabric      brown  344.0  \n",
      "4        fabric      brown  289.0  \n",
      "5        fabric      brown  266.0  \n",
      "6       leather      black  395.0  \n",
      "7       leather      white  241.0  \n",
      "8       leather       navy  266.0  \n",
      "9  faux-leather      brown  440.0  \n",
      "493.0\n",
      "5\n",
      "shoe_type\n",
      "ballet flats    481.0\n",
      "boots           478.0\n",
      "clogs           493.0\n",
      "sandles         456.0\n",
      "stilettos       487.0\n",
      "wedges          461.0\n",
      "Name: price, dtype: float64\n",
      "<class 'pandas.core.series.Series'>\n",
      "      shoe_type  price\n",
      "0  ballet flats  481.0\n",
      "1         boots  478.0\n",
      "2         clogs  493.0\n",
      "3       sandles  456.0\n",
      "4     stilettos  487.0\n",
      "5        wedges  461.0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "  shoe_color  price\n",
      "0      black    NaN\n",
      "1      brown  193.5\n",
      "2       navy  205.5\n",
      "3        red  250.0\n",
      "4      white  196.0\n",
      "       shoe_type shoe_color  id\n",
      "0   ballet flats      black   2\n",
      "1   ballet flats      brown   5\n",
      "2   ballet flats        red   3\n",
      "3   ballet flats      white   5\n",
      "4          boots      black   3\n",
      "5          boots      brown   5\n",
      "6          boots       navy   6\n",
      "7          boots        red   2\n",
      "8          boots      white   3\n",
      "9          clogs      black   4\n",
      "10         clogs      brown   6\n",
      "11         clogs       navy   1\n",
      "12         clogs        red   4\n",
      "13         clogs      white   1\n",
      "14       sandles      black   1\n",
      "15       sandles      brown   4\n",
      "16       sandles       navy   5\n",
      "17       sandles        red   3\n",
      "18       sandles      white   4\n",
      "19     stilettos      black   5\n",
      "20     stilettos      brown   3\n",
      "21     stilettos       navy   2\n",
      "22     stilettos        red   2\n",
      "23     stilettos      white   2\n",
      "24        wedges      black   3\n",
      "25        wedges      brown   4\n",
      "26        wedges       navy   4\n",
      "27        wedges        red   5\n",
      "28        wedges      white   2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:4291: RuntimeWarning: Invalid value encountered in percentile\n",
      "  interpolation=interpolation)\n"
     ]
    }
   ],
   "source": [
    "orders = pd.read_csv('unit2_orders.csv')\n",
    "print(orders.head(10))\n",
    "most_expensive = orders.price.max()\n",
    "num_colors = orders.shoe_color.nunique()\n",
    "print(most_expensive)\n",
    "print(num_colors)\n",
    "\n",
    "pricey_shoes = orders.groupby('shoe_type').price.max()\n",
    "print(pricey_shoes)\n",
    "print(type(pricey_shoes)) # not a DataFrame\n",
    "\n",
    "pricey_shoes = orders.groupby('shoe_type').price.max().reset_index()\n",
    "print(pricey_shoes)\n",
    "print(type(pricey_shoes)) # is a DataFrame\n",
    "\n",
    "# Use the apply method and lambda functions\n",
    "import numpy as np\n",
    "cheap_shoes = orders.groupby('shoe_color').price.apply(lambda x: np.percentile(x, 25)).reset_index()\n",
    "print(cheap_shoes)\n",
    "\n",
    "# Group by more than one column\n",
    "shoe_counts = orders.groupby(['shoe_type', 'shoe_color']).id.count().reset_index()\n",
    "print(shoe_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pivot table\n",
    "`df.pivot(columns='ColumnToPivot',\n",
    "         index='ColumnToBeRows',\n",
    "         values='ColumnToBeValues')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shoe_color     shoe_type  black  brown  navy  red  white\n",
      "0           ballet flats    2.0    5.0   NaN  3.0    5.0\n",
      "1                  boots    3.0    5.0   6.0  2.0    3.0\n",
      "2                  clogs    4.0    6.0   1.0  4.0    1.0\n",
      "3                sandles    1.0    4.0   5.0  3.0    4.0\n",
      "4              stilettos    5.0    3.0   2.0  2.0    2.0\n",
      "5                 wedges    3.0    4.0   4.0  5.0    2.0\n"
     ]
    }
   ],
   "source": [
    "orders = pd.read_csv('unit2_orders.csv')\n",
    "\n",
    "shoe_counts = orders.groupby(['shoe_type', 'shoe_color']).id.count().reset_index()\n",
    "shoe_counts_pivot = shoe_counts.pivot(columns='shoe_color',\n",
    "index='shoe_type',values='id').reset_index()\n",
    "print(shoe_counts_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id first_name last_name                       email         month  \\\n",
      "0  10043      Louis      Koch       LouisKoch43@gmail.com     3 - March   \n",
      "1  10150      Bruce      Webb     BruceWebb44@outlook.com     3 - March   \n",
      "2  10155   Nicholas   Hoffman  Nicholas.Hoffman@gmail.com  2 - February   \n",
      "3  10178    William       Key     William.Key@outlook.com     3 - March   \n",
      "4  10208      Karen      Bass            KB4971@gmail.com  2 - February   \n",
      "\n",
      "  utm_source  \n",
      "0      yahoo  \n",
      "1    twitter  \n",
      "2     google  \n",
      "3      yahoo  \n",
      "4     google  \n",
      "  utm_source   id\n",
      "0      email  462\n",
      "1   facebook  823\n",
      "2     google  543\n",
      "3    twitter  415\n",
      "4      yahoo  757\n",
      "month utm_source  1 - January  2 - February  3 - March\n",
      "0          email           43           147        272\n",
      "1       facebook          404           263        156\n",
      "2         google          127           196        220\n",
      "3        twitter          164           154         97\n",
      "4          yahoo          262           240        255\n"
     ]
    }
   ],
   "source": [
    "user_visits = pd.read_csv('unit2_page_visits.csv')\n",
    "# 1\n",
    "print(user_visits.head())\n",
    "# 2\n",
    "click_source = user_visits.groupby('utm_source').id.count().reset_index()\n",
    "# 3\n",
    "print(click_source)\n",
    "# 4\n",
    "click_source_by_month = user_visits .groupby(['utm_source','month']).id.count().reset_index()\n",
    "click_source_by_month_pivot = click_source_by_month.pivot(columns=\"month\",index='utm_source',values='id').reset_index()\n",
    "print(click_source_by_month_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 2 Day 9 & 10\n",
    "## Project: A/B Testing for ShoeFly.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                user_id utm_source           day  \\\n",
      "0  008b7c6c-7272-471e-b90e-930d548bd8d7     google  6 - Saturday   \n",
      "1  009abb94-5e14-4b6c-bb1c-4f4df7aa7557   facebook    7 - Sunday   \n",
      "2  00f5d532-ed58-4570-b6d2-768df5f41aed    twitter   2 - Tuesday   \n",
      "3  011adc64-0f44-4fd9-a0bb-f1506d2ad439     google   2 - Tuesday   \n",
      "4  012137e6-7ae7-4649-af68-205b4702169c   facebook    7 - Sunday   \n",
      "\n",
      "  ad_click_timestamp experimental_group  \n",
      "0               7:18                  A  \n",
      "1                NaN                  B  \n",
      "2                NaN                  A  \n",
      "3                NaN                  B  \n",
      "4                NaN                  B  \n",
      "  experimental_group  user_id\n",
      "0                  A      827\n",
      "1                  B      827\n",
      "is_click            False  True \n",
      "experimental_group              \n",
      "A                     517    310\n",
      "B                     572    255\n",
      "is_click       False  True  percent_clicked\n",
      "day                                        \n",
      "1 - Monday        70    43         0.380531\n",
      "2 - Tuesday       76    43         0.361345\n",
      "3 - Wednesday     86    38         0.306452\n",
      "4 - Thursday      69    47         0.405172\n",
      "5 - Friday        77    51         0.398438\n",
      "6 - Saturday      73    45         0.381356\n",
      "7 - Sunday        66    43         0.394495\n",
      "is_click       False  True  percent_clicked\n",
      "day                                        \n",
      "1 - Monday        81    32         0.283186\n",
      "2 - Tuesday       74    45         0.378151\n",
      "3 - Wednesday     89    35         0.282258\n",
      "4 - Thursday      87    29         0.250000\n",
      "5 - Friday        90    38         0.296875\n",
      "6 - Saturday      76    42         0.355932\n",
      "7 - Sunday        75    34         0.311927\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ad_clicks = pd.read_csv('unit2_ad_clicks.csv')\n",
    "# 1\n",
    "print(ad_clicks.head())\n",
    "# 2\n",
    "ad_clicks.groupby('utm_source').user_id.count().reset_index()\n",
    "#print(ad_clicks.groupby('utm_source').user_id.count().reset_index())\n",
    "# 3\n",
    "ad_clicks['is_click'] = ~ad_clicks.ad_click_timestamp.isnull()\n",
    "#print(ad_clicks)\n",
    "# 4\n",
    "clicks_by_source = ad_clicks.groupby(['utm_source', 'is_click']).user_id.count().reset_index()\n",
    "#print(clicks_by_source)\n",
    "# 5\n",
    "clicks_pivot = clicks_by_source.pivot(columns='is_click',index='utm_source',values='user_id').reset_index()\n",
    "#print(clicks_pivot)\n",
    "# 6\n",
    "clicks_pivot['percent_clicked'] = clicks_pivot[True] / (clicks_pivot[True] + clicks_pivot[False])\n",
    "#print(clicks_pivot)\n",
    "# 7\n",
    "print(ad_clicks.groupby('experimental_group').user_id.count().reset_index())\n",
    "# 8\n",
    "#print(ad_clicks.groupby(['experimental_group', 'is_click']).user_id.count().reset_index())\n",
    "ab_test = ad_clicks.groupby(['experimental_group', 'is_click']).user_id.count().reset_index()\n",
    "ab_test_pivot = ab_test.pivot(columns='is_click',index='experimental_group',values='user_id')\n",
    "print(ab_test_pivot)\n",
    "# 9\n",
    "a_clicks = ad_clicks[ad_clicks.experimental_group == 'A']\n",
    "b_clicks = ad_clicks[ad_clicks.experimental_group == 'B']\n",
    "#print(a_clicks)\n",
    "#print(b_clicks)\n",
    "# 10\n",
    "# group a\n",
    "a_clicks_group = a_clicks.groupby(['is_click', 'day']).user_id.count().reset_index()\n",
    "#print(a_clicks_group)\n",
    "a_clicks_group_pivot = a_clicks_group.pivot(columns='is_click',index='day',values='user_id')\n",
    "#print(a_clicks_group_pivot)\n",
    "a_clicks_group_pivot['percent_clicked'] = a_clicks_group_pivot[True] / (a_clicks_group_pivot[True] + a_clicks_group_pivot[False])\n",
    "print(a_clicks_group_pivot)\n",
    "# group b\n",
    "b_clicks_group = b_clicks.groupby(['is_click', 'day']).user_id.count().reset_index()\n",
    "#print(b_clicks_group)\n",
    "b_clicks_group_pivot = b_clicks_group.pivot(columns='is_click',index='day',values='user_id')\n",
    "#print(b_clicks_group_pivot)\n",
    "b_clicks_group_pivot['percent_clicked'] = b_clicks_group_pivot[True] / (b_clicks_group_pivot[True] + b_clicks_group_pivot[False])\n",
    "print(b_clicks_group_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 2 Day 11\n",
    "## Working with Multiple Tables in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   order_id  customer_id  product_id  quantity   timestamp\n",
      "0         1            2           3         1  2017-01-01\n",
      "1         2            2           2         3  2017-01-01\n",
      "2         3            3           1         1  2017-01-01\n",
      "3         4            3           2         2  2017-02-01\n",
      "4         5            3           3         3  2017-02-01\n",
      "5         6            1           4         2  2017-03-01\n",
      "6         7            1           1         1  2017-02-02\n",
      "7         8            1           4         1  2017-02-02\n",
      "   product_id         description  price\n",
      "0           1      thing-a-ma-jig      5\n",
      "1           2  whatcha-ma-call-it     10\n",
      "2           3          doo-hickey      7\n",
      "3           4               gizmo      3\n",
      "   customer_id customer_name        address  phone_number\n",
      "0            1    John Smith   123 Main St.  212-123-4567\n",
      "1            2      Jane Doe  456 Park Ave.  949-867-5309\n",
      "2            3     Joe Schmo   798 Broadway  112-358-1321\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "orders = pd.read_csv('unit2_orders2.csv')\n",
    "products = pd.read_csv('unit2_products2.csv')\n",
    "customers = pd.read_csv('unit2_customers.csv')\n",
    "\n",
    "print(orders)\n",
    "print(products)\n",
    "print(customers)\n",
    "\n",
    "order_3_description = 'thing-a-ma-jig'\n",
    "order_5_phone_number = '112-358-1321'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Merge\n",
    "* Use `pd.merge(DataFrame1, DataFrame2)`\n",
    "* Use `DataFram1.merge(DataFrame2).merge(DataFrame3)`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      month  revenue\n",
      "0   January      300\n",
      "1  February      290\n",
      "2     March      310\n",
      "3     April      325\n",
      "4       May      475\n",
      "5      June      495\n",
      "      month  target\n",
      "0   January     310\n",
      "1  February     270\n",
      "2     March     300\n",
      "3     April     350\n",
      "4       May     475\n",
      "5      June     500\n",
      "      month  revenue  target\n",
      "0   January      300     310\n",
      "1  February      290     270\n",
      "2     March      310     300\n",
      "3     April      325     350\n",
      "4       May      475     475\n",
      "5      June      495     500\n",
      "      month  revenue  target\n",
      "1  February      290     270\n",
      "2     March      310     300\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sales = pd.read_csv('unit2_sales.csv')\n",
    "print(sales)\n",
    "targets = pd.read_csv('unit2_targets.csv')\n",
    "print(targets)\n",
    "sales_vs_targets = pd.merge(sales, targets)\n",
    "print(sales_vs_targets)\n",
    "crushing_it = sales_vs_targets[sales_vs_targets.revenue > sales_vs_targets.target]\n",
    "print(crushing_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      month  revenue\n",
      "0   January      300\n",
      "1  February      290\n",
      "2     March      310\n",
      "3     April      325\n",
      "4       May      475\n",
      "5      June      495\n",
      "      month  target\n",
      "0   January     310\n",
      "1  February     270\n",
      "2     March     300\n",
      "3     April     350\n",
      "4       May     475\n",
      "5      June     500\n",
      "      month  revenue  target  men  women\n",
      "0   January      300     310   30     35\n",
      "1  February      290     270   29     35\n",
      "2     March      310     300   31     29\n",
      "3     April      325     350   32     28\n",
      "4       May      475     475   47     50\n",
      "5      June      495     500   49     45\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sales = pd.read_csv('unit2_sales.csv')\n",
    "print(sales)\n",
    "targets = pd.read_csv('unit2_targets.csv')\n",
    "print(targets)\n",
    "men_women = pd.read_csv('unit2_men_women_sales.csv')\n",
    "all_data = sales.merge(targets).merge(men_women)\n",
    "print(all_data)\n",
    "results = all_data[(all_data.revenue > all_data.target) & (all_data.women > all_data.men)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge on Specific Columns\n",
    "* Use `pd.merge(DataFrame1, DataFramd2.rename(columns={'old_name': 'new_name'}))`\n",
    "* Use `pd.merge(DataFrame1, DataFramd2, left_on='name_in_DataFrame1', right_on='name_in_DataFrame2', suffixes=['_suffix1', '_suffix2'])`\n",
    "\n",
    "### Outer Merge\n",
    "* An Outer Join would include all rows from both tables, even if they don't match.\n",
    "  * Any missing values are filled in with None or nan (which stands for \"Not a Number\").\n",
    "* Use `pd.merge(DataFrame1, DataFramd2, how='outer')`\n",
    "\n",
    "### Left Merge\n",
    "* A Left Merge includes all rows from the first (left) table, but only rows from the second (right) table that match the first table.\n",
    "* Use `pd.merge(DataFrame1, DataFramd2, how='left')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  product_id  customer_id  quantity   timestamp\n",
      "0   1           3            2         1  2017-01-01\n",
      "1   2           2            2         3  2017-01-01\n",
      "2   3           1            3         1  2017-01-01\n",
      "3   4           2            3         2  2016-02-01\n",
      "4   5           3            3         3  2017-02-01\n",
      "5   6           4            1         2  2017-03-01\n",
      "6   7           1            1         1  2017-02-02\n",
      "7   8           4            1         1  2017-02-02\n",
      "   id         description  price\n",
      "0   1      thing-a-ma-jig      5\n",
      "1   2  whatcha-ma-call-it     10\n",
      "2   3          doo-hickey      7\n",
      "3   4               gizmo      3\n",
      "   id  product_id  customer_id  quantity   timestamp         description  \\\n",
      "0   1           3            2         1  2017-01-01          doo-hickey   \n",
      "1   5           3            3         3  2017-02-01          doo-hickey   \n",
      "2   2           2            2         3  2017-01-01  whatcha-ma-call-it   \n",
      "3   4           2            3         2  2016-02-01  whatcha-ma-call-it   \n",
      "4   3           1            3         1  2017-01-01      thing-a-ma-jig   \n",
      "5   7           1            1         1  2017-02-02      thing-a-ma-jig   \n",
      "6   6           4            1         2  2017-03-01               gizmo   \n",
      "7   8           4            1         1  2017-02-02               gizmo   \n",
      "\n",
      "   price  \n",
      "0      7  \n",
      "1      7  \n",
      "2     10  \n",
      "3     10  \n",
      "4      5  \n",
      "5      5  \n",
      "6      3  \n",
      "7      3  \n"
     ]
    }
   ],
   "source": [
    "orders = pd.read_csv('unit2_orders3.csv')\n",
    "print(orders)\n",
    "products = pd.read_csv('unit2_products3.csv')\n",
    "print(products)\n",
    "orders_products = pd.merge(orders, products.rename(columns={'id':'product_id'}))\n",
    "print(orders_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  product_id  customer_id  quantity   timestamp\n",
      "0   1           3            2         1  2017-01-01\n",
      "1   2           2            2         3  2017-01-01\n",
      "2   3           1            3         1  2017-01-01\n",
      "3   4           2            3         2  2016-02-01\n",
      "4   5           3            3         3  2017-02-01\n",
      "5   6           4            1         2  2017-03-01\n",
      "6   7           1            1         1  2017-02-02\n",
      "7   8           4            1         1  2017-02-02\n",
      "   id         description  price\n",
      "0   1      thing-a-ma-jig      5\n",
      "1   2  whatcha-ma-call-it     10\n",
      "2   3          doo-hickey      7\n",
      "3   4               gizmo      3\n",
      "   id_orders  product_id  customer_id  quantity   timestamp  id_products  \\\n",
      "0          1           3            2         1  2017-01-01            3   \n",
      "1          5           3            3         3  2017-02-01            3   \n",
      "2          2           2            2         3  2017-01-01            2   \n",
      "3          4           2            3         2  2016-02-01            2   \n",
      "4          3           1            3         1  2017-01-01            1   \n",
      "5          7           1            1         1  2017-02-02            1   \n",
      "6          6           4            1         2  2017-03-01            4   \n",
      "7          8           4            1         1  2017-02-02            4   \n",
      "\n",
      "          description  price  \n",
      "0          doo-hickey      7  \n",
      "1          doo-hickey      7  \n",
      "2  whatcha-ma-call-it     10  \n",
      "3  whatcha-ma-call-it     10  \n",
      "4      thing-a-ma-jig      5  \n",
      "5      thing-a-ma-jig      5  \n",
      "6               gizmo      3  \n",
      "7               gizmo      3  \n"
     ]
    }
   ],
   "source": [
    "orders = pd.read_csv('unit2_orders4.csv')\n",
    "print(orders)\n",
    "products = pd.read_csv('unit2_products4.csv')\n",
    "print(products)\n",
    "orders_products = pd.merge(orders,products,left_on='product_id',right_on='id', suffixes=['_orders','_products'])\n",
    "print(orders_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  product_id  customer_id  quantity   timestamp\n",
      "0   1           3            2         1  2017-01-01\n",
      "1   2           2            2         3  2017-01-01\n",
      "2   3           5            1         1  2017-01-01\n",
      "3   4           2            3         2  2016-02-01\n",
      "4   5           3            3         3  2017-02-01\n",
      "   product_id         description  price\n",
      "0           1      thing-a-ma-jig      5\n",
      "1           2  whatcha-ma-call-it     10\n",
      "2           3          doo-hickey      7\n",
      "3           4               gizmo      3\n",
      "   id  product_id  customer_id  quantity   timestamp         description  \\\n",
      "0   1           3            2         1  2017-01-01          doo-hickey   \n",
      "1   5           3            3         3  2017-02-01          doo-hickey   \n",
      "2   2           2            2         3  2017-01-01  whatcha-ma-call-it   \n",
      "3   4           2            3         2  2016-02-01  whatcha-ma-call-it   \n",
      "\n",
      "   price  \n",
      "0      7  \n",
      "1      7  \n",
      "2     10  \n",
      "3     10  \n"
     ]
    }
   ],
   "source": [
    "# Mismatched Merges\n",
    "orders = pd.read_csv('unit2_orders5.csv')\n",
    "products = pd.read_csv('unit2_products5.csv')\n",
    "print(orders)\n",
    "print(products)\n",
    "merged_df = pd.merge(orders,products)\n",
    "print(merged_df) # only merge the rows exist in both data frame\n",
    "# when we merge two DataFrames whose rows don't match perfectly, we lose the unmatched rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          item  store_a_inventory\n",
      "0       hammer                 12\n",
      "1  screwdriver                 15\n",
      "2        nails                200\n",
      "3       screws                350\n",
      "4          saw                  6\n",
      "5    duct tape                150\n",
      "6       wrench                 12\n",
      "7     pvc pipe                 54\n",
      "            item  store_b_inventory\n",
      "0         hammer                  6\n",
      "1          nails                250\n",
      "2            saw                  6\n",
      "3      duct tape                150\n",
      "4       pvc pipe                 54\n",
      "5           rake                 10\n",
      "6         shovel                 15\n",
      "7  wooden dowels                192\n",
      "             item  store_a_inventory  store_b_inventory\n",
      "0          hammer               12.0                6.0\n",
      "1     screwdriver               15.0                NaN\n",
      "2           nails              200.0              250.0\n",
      "3          screws              350.0                NaN\n",
      "4             saw                6.0                6.0\n",
      "5       duct tape              150.0              150.0\n",
      "6          wrench               12.0                NaN\n",
      "7        pvc pipe               54.0               54.0\n",
      "8            rake                NaN               10.0\n",
      "9          shovel                NaN               15.0\n",
      "10  wooden dowels                NaN              192.0\n"
     ]
    }
   ],
   "source": [
    "store_a = pd.read_csv('unit2_store_a.csv')\n",
    "print(store_a)\n",
    "store_b = pd.read_csv('unit2_store_b.csv')\n",
    "print(store_b)\n",
    "store_a_b_outer = pd.merge(store_a, store_b,how='outer')\n",
    "print(store_a_b_outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          item  store_a_inventory\n",
      "0       hammer                 12\n",
      "1  screwdriver                 15\n",
      "2        nails                200\n",
      "3       screws                350\n",
      "4          saw                  6\n",
      "5    duct tape                150\n",
      "6       wrench                 12\n",
      "7     pvc pipe                 54\n",
      "            item  store_b_inventory\n",
      "0         hammer                  6\n",
      "1          nails                250\n",
      "2            saw                  6\n",
      "3      duct tape                150\n",
      "4       pvc pipe                 54\n",
      "5           rake                 10\n",
      "6         shovel                 15\n",
      "7  wooden dowels                192\n",
      "          item  store_a_inventory  store_b_inventory\n",
      "0       hammer                 12                6.0\n",
      "1  screwdriver                 15                NaN\n",
      "2        nails                200              250.0\n",
      "3       screws                350                NaN\n",
      "4          saw                  6                6.0\n",
      "5    duct tape                150              150.0\n",
      "6       wrench                 12                NaN\n",
      "7     pvc pipe                 54               54.0\n",
      "            item  store_b_inventory  store_a_inventory\n",
      "0         hammer                  6               12.0\n",
      "1          nails                250              200.0\n",
      "2            saw                  6                6.0\n",
      "3      duct tape                150              150.0\n",
      "4       pvc pipe                 54               54.0\n",
      "5           rake                 10                NaN\n",
      "6         shovel                 15                NaN\n",
      "7  wooden dowels                192                NaN\n"
     ]
    }
   ],
   "source": [
    "store_a = pd.read_csv('unit2_store_a.csv')\n",
    "print(store_a)\n",
    "store_b = pd.read_csv('unit2_store_b.csv')\n",
    "print(store_b)\n",
    "# 1\n",
    "store_a_b_left = pd.merge(store_a,store_b,how='left')\n",
    "print(store_a_b_left)\n",
    "# 2\n",
    "store_b_a_left = pd.merge(store_b, store_a,how='left')\n",
    "print(store_b_a_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate DataFrames\n",
    "* Use `pd.concat([df1, df2, df2, ...])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  item  price\n",
      "0               cookie   2.50\n",
      "1              brownie   3.50\n",
      "2        slice of cake   4.75\n",
      "3  slice of cheesecake   4.75\n",
      "4         slice of pie   5.00\n",
      "                              item  price\n",
      "0     scoop of chocolate ice cream   3.00\n",
      "1       scoop of vanilla ice cream   2.95\n",
      "2    scoop of strawberry ice cream   3.05\n",
      "3  scoop of cookie dough ice cream   3.25\n",
      "                              item  price\n",
      "0                           cookie   2.50\n",
      "1                          brownie   3.50\n",
      "2                    slice of cake   4.75\n",
      "3              slice of cheesecake   4.75\n",
      "4                     slice of pie   5.00\n",
      "0     scoop of chocolate ice cream   3.00\n",
      "1       scoop of vanilla ice cream   2.95\n",
      "2    scoop of strawberry ice cream   3.05\n",
      "3  scoop of cookie dough ice cream   3.25\n"
     ]
    }
   ],
   "source": [
    "bakery = pd.read_csv('unit2_bakery.csv')\n",
    "print(bakery)\n",
    "ice_cream = pd.read_csv('unit2_ice_cream.csv')\n",
    "print(ice_cream)\n",
    "menu = pd.concat([bakery,ice_cream])\n",
    "print(menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 user_id          visit_time\n",
      "0   319350b4-9951-47ef-b3a7-6b252099905f 2017-02-21 07:16:00\n",
      "1   7435ec9f-576d-4ebd-8791-361b128fca77 2017-05-16 08:37:00\n",
      "2   0b061e73-f709-42fa-8d1a-5f68176ff154 2017-04-12 19:32:00\n",
      "3   9133d6f0-e68b-4c8d-bafd-ff2825e8dafe 2017-08-18 04:32:00\n",
      "4   08d13edb-071c-4cfb-9ee4-8f377d0e932a 2017-07-08 06:24:00\n",
      "5   c7192ab9-e033-4b69-971d-4bd92631342e 2017-10-05 09:16:00\n",
      "6   c4dac0f2-2fa9-48a8-b056-c3b2a5a5c683 2017-07-09 14:19:00\n",
      "7   f028e9dd-77d0-4002-83f6-372a4837fda6 2017-10-27 08:46:00\n",
      "8   e43cf28f-7d08-4019-bd66-ddf7dfd2e034 2017-11-12 01:47:00\n",
      "9   746631d2-35d5-441e-a21b-e5f39442f981 2017-06-19 23:34:00\n",
      "10  a0fc94a2-4a80-4a33-994b-75783066ac62 2017-05-11 13:07:00\n",
      "11  e2c24ee0-7fdf-4400-abde-b36378fe5ce6 2017-07-04 15:33:00\n",
      "12  78751233-c0de-44fb-bc2f-822bd9dd9be7 2017-01-23 05:38:00\n",
      "13  fbcec4bc-f191-4c0c-870b-d22728ad1b18 2017-01-24 17:41:00\n",
      "14  e6c7ecb9-4710-4cbd-ad02-c43971ebbe7f 2017-09-27 16:10:00\n",
      "15  0c682ddd-144a-4743-9c82-dbd942fdea52 2017-08-15 23:38:00\n",
      "16  fe07fc99-3943-4000-99a5-422957a42ea1 2017-10-07 15:08:00\n",
      "17  bb6d2daf-bcf2-4970-b45d-33402cc1c45d 2017-07-04 21:22:00\n",
      "18  ad7a3539-ad48-4b0e-bcc3-8c0dff722908 2017-06-13 08:28:00\n",
      "19  10dbd3c5-d610-44e9-9994-110a7950b6b4 2017-08-09 21:01:00\n",
      "20  9e744240-18e8-4da0-9e05-89b116245c15 2017-11-02 20:01:00\n",
      "21  b2b04aa6-3ef1-46d6-939b-215126b4b91b 2017-05-03 13:15:00\n",
      "22  f8123e12-d349-4efe-ae65-9494630bee6c 2017-02-22 13:16:00\n",
      "23  6aee3eda-65f2-437f-88b2-cf7e2b94fcdc 2017-10-17 10:17:00\n",
      "24  ee9f8ae4-5450-4c92-ae54-edd44cd0482e 2017-05-12 14:29:00\n",
      "25  0d798d59-fb81-40ae-8ab1-d4beaffe8715 2017-01-18 17:26:00\n",
      "26  f5f90dcf-15a0-432b-9886-9e4b5907a1cc 2017-07-10 11:39:00\n",
      "27  c54a72a7-42db-4d19-b897-944f37cf386d 2017-01-02 17:20:00\n",
      "28  d7a50dcf-e6d7-44af-87a7-9cc68f303ece 2017-11-05 08:15:00\n",
      "29  65599d0d-76c2-4ad4-a717-70f12a187f1a 2017-02-15 07:21:00\n",
      "..                                   ...                 ...\n",
      "70  280c9dcb-2c9f-4f33-ada1-92196c0b1d37 2017-04-10 14:55:00\n",
      "71  f74519df-e961-4841-acdb-2d47da194aba 2017-11-17 21:05:00\n",
      "72  7fe800cc-46e8-427c-a7af-f27198d305a1 2017-01-18 12:50:00\n",
      "73  0fbf570b-bc50-4f9c-ad63-9070e7d888b6 2017-08-08 23:50:00\n",
      "74  6a504e55-b59c-416d-beda-4194c89066e6 2017-02-25 15:54:00\n",
      "75  fe90a9f4-960a-4a0d-9160-e562adb79365 2017-11-09 09:04:00\n",
      "76  52b650a4-f315-4947-804f-19df5f971d85 2017-07-05 21:15:00\n",
      "77  9812465d-fded-43c4-8685-3e96446f6cc7 2017-07-19 17:02:00\n",
      "78  66b4a07f-224b-4f83-91b2-b2d27c9dbe73 2017-06-23 14:17:00\n",
      "79  f41d2868-515d-49a2-b48c-e4f33e5d9b69 2017-09-07 16:40:00\n",
      "80  e0e3191d-4adb-4b43-8815-0ab43815c4d7 2017-11-27 16:30:00\n",
      "81  43db76fc-d522-450d-a371-ef2a683d5bfd 2017-03-26 21:11:00\n",
      "82  1a35b7eb-f603-407d-91be-a2c3304066fd 2017-08-15 21:09:00\n",
      "83  27041dde-19da-4571-9a69-46ca3619b44c 2017-09-08 12:12:00\n",
      "84  b9644df3-e2f6-4226-8df1-ac1945952494 2017-03-16 10:13:00\n",
      "85  8b4dbf70-db73-4bc6-88fe-fc8aaeae9277 2017-11-22 15:02:00\n",
      "86  b7953447-00a8-42be-99d2-b511f4e9c12b 2017-04-24 10:13:00\n",
      "87  2f1e93f2-4d40-45e1-8371-7c65660f6bf9 2017-10-25 19:42:00\n",
      "88  2d747792-db15-4549-abb8-ef352884b3db 2017-01-16 18:10:00\n",
      "89  23a8d1be-3f5c-4b59-aed7-c7f19c51612b 2017-08-11 13:49:00\n",
      "90  c09b76e2-3a95-426c-ae76-c1e0f6a15aee 2017-06-07 12:24:00\n",
      "91  2da32318-0fd3-4432-85da-cb55e3bdd2ec 2017-07-16 10:11:00\n",
      "92  a9def5d7-dba9-4175-a2da-98c2d2854984 2017-08-11 07:29:00\n",
      "93  41efeead-9983-45f3-bec1-44887a03f6ab 2017-09-26 14:22:00\n",
      "94  8d9ac96c-16be-418e-8df4-1a6202d0b36e 2017-10-07 10:23:00\n",
      "95  442efd1c-8d7b-4d6a-83be-8f2a9e08b34f 2017-02-19 11:20:00\n",
      "96  5679519b-a901-4970-8656-dbf60ffb618d 2017-07-20 04:23:00\n",
      "97  26deb2d5-1d7e-4774-bf6e-1df2ee9ee59d 2017-09-06 07:29:00\n",
      "98  fff8f87a-e4a2-4f2c-b3d4-93a4ece95c4f 2017-06-06 23:42:00\n",
      "99  42617776-2850-431d-9262-a9c11e5ad17f 2017-02-10 10:49:00\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "                                 user_id       checkout_time\n",
      "0   fe90a9f4-960a-4a0d-9160-e562adb79365 2017-11-09 09:25:00\n",
      "1   1a35b7eb-f603-407d-91be-a2c3304066fd 2017-08-15 21:25:00\n",
      "2   e2c24ee0-7fdf-4400-abde-b36378fe5ce6 2017-07-04 15:39:00\n",
      "3   10dbd3c5-d610-44e9-9994-110a7950b6b4 2017-08-09 21:07:00\n",
      "4   f028e9dd-77d0-4002-83f6-372a4837fda6 2017-10-27 08:57:00\n",
      "5   b7953447-00a8-42be-99d2-b511f4e9c12b 2017-04-24 10:20:00\n",
      "6   280c9dcb-2c9f-4f33-ada1-92196c0b1d37 2017-04-10 14:57:00\n",
      "7   319350b4-9951-47ef-b3a7-6b252099905f 2017-02-21 07:27:00\n",
      "8   08d13edb-071c-4cfb-9ee4-8f377d0e932a 2017-07-08 06:32:00\n",
      "9   cb602c66-1366-467a-8bee-52477310cf42 2017-07-03 06:32:00\n",
      "10  62173c69-bec8-4a5d-951c-b860df94adc2 2017-05-04 00:08:00\n",
      "11  34b2bf31-9ce0-4bb7-8f18-624ff4f4e904 2017-02-13 15:39:00\n",
      "12  2d747792-db15-4549-abb8-ef352884b3db 2017-01-16 18:30:00\n",
      "13  ccd0db8b-5e94-4103-a0b0-14ba5289c6f3 2017-08-19 06:12:00\n",
      "14  7435ec9f-576d-4ebd-8791-361b128fca77 2017-05-16 08:49:00\n",
      "15  147d51cd-95d7-48f7-871d-112e7e7f47fb 2017-06-09 14:09:00\n",
      "16  2f1e93f2-4d40-45e1-8371-7c65660f6bf9 2017-10-25 19:49:00\n",
      "17  f8123e12-d349-4efe-ae65-9494630bee6c 2017-02-22 13:44:00\n",
      "18  fe07fc99-3943-4000-99a5-422957a42ea1 2017-10-07 15:25:00\n",
      "19  2f1e93f2-4d40-45e1-8371-7c65660f6bf9 2017-10-25 20:05:00\n",
      "20  ad7a3539-ad48-4b0e-bcc3-8c0dff722908 2017-06-13 08:40:00\n",
      "21  b2b04aa6-3ef1-46d6-939b-215126b4b91b 2017-05-03 13:19:00\n",
      "22  5679519b-a901-4970-8656-dbf60ffb618d 2017-07-20 04:24:00\n",
      "23  fff8f87a-e4a2-4f2c-b3d4-93a4ece95c4f 2017-06-07 00:11:00\n",
      "24  0d798d59-fb81-40ae-8ab1-d4beaffe8715 2017-01-18 17:48:00\n",
      "25  147d51cd-95d7-48f7-871d-112e7e7f47fb 2017-06-09 14:09:00\n",
      "26  10dbd3c5-d610-44e9-9994-110a7950b6b4 2017-08-09 21:04:00\n",
      "27  9e744240-18e8-4da0-9e05-89b116245c15 2017-11-02 20:04:00\n",
      "28  d2ae16fa-abc6-4755-af80-5010a7c6a103 2017-04-19 03:21:00\n",
      "29  851b52d1-31e9-468d-834f-5363fee108ac 2017-09-21 22:06:00\n",
      "..                                   ...                 ...\n",
      "50  3a46dc74-4505-4a1a-8dde-4268a7484321 2017-09-17 23:00:00\n",
      "51  23a8d1be-3f5c-4b59-aed7-c7f19c51612b 2017-08-11 14:11:00\n",
      "52  8d9ac96c-16be-418e-8df4-1a6202d0b36e 2017-10-07 10:24:00\n",
      "53  cc90d623-e2d8-468b-aa8b-912d7b9b67c8 2017-02-17 18:22:00\n",
      "54  8d9ac96c-16be-418e-8df4-1a6202d0b36e 2017-10-07 10:52:00\n",
      "55  62173c69-bec8-4a5d-951c-b860df94adc2 2017-05-04 00:07:00\n",
      "56  cc90d623-e2d8-468b-aa8b-912d7b9b67c8 2017-02-17 18:27:00\n",
      "57  0d798d59-fb81-40ae-8ab1-d4beaffe8715 2017-01-18 17:38:00\n",
      "58  34b2bf31-9ce0-4bb7-8f18-624ff4f4e904 2017-02-13 15:25:00\n",
      "59  6d746f96-30eb-49df-8131-9ab61e8b9b20 2017-06-20 07:54:00\n",
      "60  287c13f5-9a9a-4c3a-9f0d-13853e8aad9b 2017-03-27 23:44:00\n",
      "61  f41d2868-515d-49a2-b48c-e4f33e5d9b69 2017-09-07 16:47:00\n",
      "62  67905d3e-5332-4927-a8ca-7ae9daabc7b8 2017-05-20 03:44:00\n",
      "63  31f1be0d-4502-4616-a7d6-a90163b8bb54 2017-11-11 03:57:00\n",
      "64  d722f212-04e9-482d-8f72-9d0df4cc8e6a 2017-02-21 20:03:00\n",
      "65  c4dac0f2-2fa9-48a8-b056-c3b2a5a5c683 2017-07-09 14:39:00\n",
      "66  3e5aa90a-279f-4ef8-99a2-9262604e9acc 2017-04-14 04:25:00\n",
      "67  7fe800cc-46e8-427c-a7af-f27198d305a1 2017-01-18 13:09:00\n",
      "68  f5f90dcf-15a0-432b-9886-9e4b5907a1cc 2017-07-10 12:03:00\n",
      "69  a0fc94a2-4a80-4a33-994b-75783066ac62 2017-05-11 13:31:00\n",
      "70  65599d0d-76c2-4ad4-a717-70f12a187f1a 2017-02-15 07:27:00\n",
      "71  b2b04aa6-3ef1-46d6-939b-215126b4b91b 2017-05-03 13:25:00\n",
      "72  c4dac0f2-2fa9-48a8-b056-c3b2a5a5c683 2017-07-09 14:42:00\n",
      "73  0b6b993f-3364-46ee-9baa-12b7fa546927 2017-03-06 23:59:00\n",
      "74  9812465d-fded-43c4-8685-3e96446f6cc7 2017-07-19 17:15:00\n",
      "75  7fe800cc-46e8-427c-a7af-f27198d305a1 2017-01-18 13:14:00\n",
      "76  43db76fc-d522-450d-a371-ef2a683d5bfd 2017-03-26 21:31:00\n",
      "77  851b52d1-31e9-468d-834f-5363fee108ac 2017-09-21 22:24:00\n",
      "78  7435ec9f-576d-4ebd-8791-361b128fca77 2017-05-16 08:55:00\n",
      "79  319350b4-9951-47ef-b3a7-6b252099905f 2017-02-21 07:40:00\n",
      "\n",
      "[80 rows x 2 columns]\n",
      "                                 user_id          visit_time  \\\n",
      "0   319350b4-9951-47ef-b3a7-6b252099905f 2017-02-21 07:16:00   \n",
      "1   319350b4-9951-47ef-b3a7-6b252099905f 2017-02-21 07:16:00   \n",
      "2   7435ec9f-576d-4ebd-8791-361b128fca77 2017-05-16 08:37:00   \n",
      "3   7435ec9f-576d-4ebd-8791-361b128fca77 2017-05-16 08:37:00   \n",
      "4   08d13edb-071c-4cfb-9ee4-8f377d0e932a 2017-07-08 06:24:00   \n",
      "5   c4dac0f2-2fa9-48a8-b056-c3b2a5a5c683 2017-07-09 14:19:00   \n",
      "6   c4dac0f2-2fa9-48a8-b056-c3b2a5a5c683 2017-07-09 14:19:00   \n",
      "7   c4dac0f2-2fa9-48a8-b056-c3b2a5a5c683 2017-07-09 14:19:00   \n",
      "8   c4dac0f2-2fa9-48a8-b056-c3b2a5a5c683 2017-07-09 14:19:00   \n",
      "9   f028e9dd-77d0-4002-83f6-372a4837fda6 2017-10-27 08:46:00   \n",
      "10  746631d2-35d5-441e-a21b-e5f39442f981 2017-06-19 23:34:00   \n",
      "11  746631d2-35d5-441e-a21b-e5f39442f981 2017-06-19 23:34:00   \n",
      "12  a0fc94a2-4a80-4a33-994b-75783066ac62 2017-05-11 13:07:00   \n",
      "13  e2c24ee0-7fdf-4400-abde-b36378fe5ce6 2017-07-04 15:33:00   \n",
      "14  e6c7ecb9-4710-4cbd-ad02-c43971ebbe7f 2017-09-27 16:10:00   \n",
      "15  fe07fc99-3943-4000-99a5-422957a42ea1 2017-10-07 15:08:00   \n",
      "16  ad7a3539-ad48-4b0e-bcc3-8c0dff722908 2017-06-13 08:28:00   \n",
      "17  ad7a3539-ad48-4b0e-bcc3-8c0dff722908 2017-06-13 08:28:00   \n",
      "18  10dbd3c5-d610-44e9-9994-110a7950b6b4 2017-08-09 21:01:00   \n",
      "19  10dbd3c5-d610-44e9-9994-110a7950b6b4 2017-08-09 21:01:00   \n",
      "20  9e744240-18e8-4da0-9e05-89b116245c15 2017-11-02 20:01:00   \n",
      "21  b2b04aa6-3ef1-46d6-939b-215126b4b91b 2017-05-03 13:15:00   \n",
      "22  b2b04aa6-3ef1-46d6-939b-215126b4b91b 2017-05-03 13:15:00   \n",
      "23  b2b04aa6-3ef1-46d6-939b-215126b4b91b 2017-05-03 13:15:00   \n",
      "24  f8123e12-d349-4efe-ae65-9494630bee6c 2017-02-22 13:16:00   \n",
      "25  0d798d59-fb81-40ae-8ab1-d4beaffe8715 2017-01-18 17:26:00   \n",
      "26  0d798d59-fb81-40ae-8ab1-d4beaffe8715 2017-01-18 17:26:00   \n",
      "27  f5f90dcf-15a0-432b-9886-9e4b5907a1cc 2017-07-10 11:39:00   \n",
      "28  f5f90dcf-15a0-432b-9886-9e4b5907a1cc 2017-07-10 11:39:00   \n",
      "29  65599d0d-76c2-4ad4-a717-70f12a187f1a 2017-02-15 07:21:00   \n",
      "..                                   ...                 ...   \n",
      "50  cb602c66-1366-467a-8bee-52477310cf42 2017-07-03 06:15:00   \n",
      "51  3a46dc74-4505-4a1a-8dde-4268a7484321 2017-09-17 22:34:00   \n",
      "52  67905d3e-5332-4927-a8ca-7ae9daabc7b8 2017-05-20 03:25:00   \n",
      "53  d2ae16fa-abc6-4755-af80-5010a7c6a103 2017-04-19 03:05:00   \n",
      "54  20fee849-260d-439f-a63c-bc2a0ebb6eb9 2017-01-04 10:43:00   \n",
      "55  010db559-8316-46b5-be65-207cecba63a6 2017-04-05 10:34:00   \n",
      "56  010db559-8316-46b5-be65-207cecba63a6 2017-04-05 10:34:00   \n",
      "57  287c13f5-9a9a-4c3a-9f0d-13853e8aad9b 2017-03-27 23:30:00   \n",
      "58  280c9dcb-2c9f-4f33-ada1-92196c0b1d37 2017-04-10 14:55:00   \n",
      "59  f74519df-e961-4841-acdb-2d47da194aba 2017-11-17 21:05:00   \n",
      "60  7fe800cc-46e8-427c-a7af-f27198d305a1 2017-01-18 12:50:00   \n",
      "61  7fe800cc-46e8-427c-a7af-f27198d305a1 2017-01-18 12:50:00   \n",
      "62  fe90a9f4-960a-4a0d-9160-e562adb79365 2017-11-09 09:04:00   \n",
      "63  52b650a4-f315-4947-804f-19df5f971d85 2017-07-05 21:15:00   \n",
      "64  9812465d-fded-43c4-8685-3e96446f6cc7 2017-07-19 17:02:00   \n",
      "65  f41d2868-515d-49a2-b48c-e4f33e5d9b69 2017-09-07 16:40:00   \n",
      "66  43db76fc-d522-450d-a371-ef2a683d5bfd 2017-03-26 21:11:00   \n",
      "67  1a35b7eb-f603-407d-91be-a2c3304066fd 2017-08-15 21:09:00   \n",
      "68  b7953447-00a8-42be-99d2-b511f4e9c12b 2017-04-24 10:13:00   \n",
      "69  b7953447-00a8-42be-99d2-b511f4e9c12b 2017-04-24 10:13:00   \n",
      "70  2f1e93f2-4d40-45e1-8371-7c65660f6bf9 2017-10-25 19:42:00   \n",
      "71  2f1e93f2-4d40-45e1-8371-7c65660f6bf9 2017-10-25 19:42:00   \n",
      "72  2d747792-db15-4549-abb8-ef352884b3db 2017-01-16 18:10:00   \n",
      "73  2d747792-db15-4549-abb8-ef352884b3db 2017-01-16 18:10:00   \n",
      "74  2d747792-db15-4549-abb8-ef352884b3db 2017-01-16 18:10:00   \n",
      "75  23a8d1be-3f5c-4b59-aed7-c7f19c51612b 2017-08-11 13:49:00   \n",
      "76  8d9ac96c-16be-418e-8df4-1a6202d0b36e 2017-10-07 10:23:00   \n",
      "77  8d9ac96c-16be-418e-8df4-1a6202d0b36e 2017-10-07 10:23:00   \n",
      "78  5679519b-a901-4970-8656-dbf60ffb618d 2017-07-20 04:23:00   \n",
      "79  fff8f87a-e4a2-4f2c-b3d4-93a4ece95c4f 2017-06-06 23:42:00   \n",
      "\n",
      "         checkout_time     time  \n",
      "0  2017-02-21 07:27:00 00:11:00  \n",
      "1  2017-02-21 07:40:00 00:24:00  \n",
      "2  2017-05-16 08:49:00 00:12:00  \n",
      "3  2017-05-16 08:55:00 00:18:00  \n",
      "4  2017-07-08 06:32:00 00:08:00  \n",
      "5  2017-07-09 14:26:00 00:07:00  \n",
      "6  2017-07-09 14:24:00 00:05:00  \n",
      "7  2017-07-09 14:39:00 00:20:00  \n",
      "8  2017-07-09 14:42:00 00:23:00  \n",
      "9  2017-10-27 08:57:00 00:11:00  \n",
      "10 2017-06-19 23:52:00 00:18:00  \n",
      "11 2017-06-19 23:47:00 00:13:00  \n",
      "12 2017-05-11 13:31:00 00:24:00  \n",
      "13 2017-07-04 15:39:00 00:06:00  \n",
      "14 2017-09-27 16:30:00 00:20:00  \n",
      "15 2017-10-07 15:25:00 00:17:00  \n",
      "16 2017-06-13 08:40:00 00:12:00  \n",
      "17 2017-06-13 08:50:00 00:22:00  \n",
      "18 2017-08-09 21:07:00 00:06:00  \n",
      "19 2017-08-09 21:04:00 00:03:00  \n",
      "20 2017-11-02 20:04:00 00:03:00  \n",
      "21 2017-05-03 13:19:00 00:04:00  \n",
      "22 2017-05-03 13:15:00 00:00:00  \n",
      "23 2017-05-03 13:25:00 00:10:00  \n",
      "24 2017-02-22 13:44:00 00:28:00  \n",
      "25 2017-01-18 17:48:00 00:22:00  \n",
      "26 2017-01-18 17:38:00 00:12:00  \n",
      "27 2017-07-10 12:08:00 00:29:00  \n",
      "28 2017-07-10 12:03:00 00:24:00  \n",
      "29 2017-02-15 07:27:00 00:06:00  \n",
      "..                 ...      ...  \n",
      "50 2017-07-03 06:37:00 00:22:00  \n",
      "51 2017-09-17 23:00:00 00:26:00  \n",
      "52 2017-05-20 03:44:00 00:19:00  \n",
      "53 2017-04-19 03:21:00 00:16:00  \n",
      "54 2017-01-04 10:58:00 00:15:00  \n",
      "55 2017-04-05 10:42:00 00:08:00  \n",
      "56 2017-04-05 10:35:00 00:01:00  \n",
      "57 2017-03-27 23:44:00 00:14:00  \n",
      "58 2017-04-10 14:57:00 00:02:00  \n",
      "59 2017-11-17 21:08:00 00:03:00  \n",
      "60 2017-01-18 13:09:00 00:19:00  \n",
      "61 2017-01-18 13:14:00 00:24:00  \n",
      "62 2017-11-09 09:25:00 00:21:00  \n",
      "63 2017-07-05 21:42:00 00:27:00  \n",
      "64 2017-07-19 17:15:00 00:13:00  \n",
      "65 2017-09-07 16:47:00 00:07:00  \n",
      "66 2017-03-26 21:31:00 00:20:00  \n",
      "67 2017-08-15 21:25:00 00:16:00  \n",
      "68 2017-04-24 10:20:00 00:07:00  \n",
      "69 2017-04-24 10:26:00 00:13:00  \n",
      "70 2017-10-25 19:49:00 00:07:00  \n",
      "71 2017-10-25 20:05:00 00:23:00  \n",
      "72 2017-01-16 18:30:00 00:20:00  \n",
      "73 2017-01-16 18:10:00 00:00:00  \n",
      "74 2017-01-16 18:39:00 00:29:00  \n",
      "75 2017-08-11 14:11:00 00:22:00  \n",
      "76 2017-10-07 10:24:00 00:01:00  \n",
      "77 2017-10-07 10:52:00 00:29:00  \n",
      "78 2017-07-20 04:24:00 00:01:00  \n",
      "79 2017-06-07 00:11:00 00:29:00  \n",
      "\n",
      "[80 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 days 00:15:24.750000\n"
     ]
    }
   ],
   "source": [
    "visits = pd.read_csv('unit2_visits.csv', parse_dates=[1])\n",
    "checkouts = pd.read_csv('unit2_checkouts.csv', parse_dates=[1])\n",
    "\n",
    "print(visits)\n",
    "print(checkouts)\n",
    "v_to_c = pd.merge(visits, checkouts)\n",
    "v_to_c['time'] = v_to_c.checkout_time - v_to_c.visit_time\n",
    "print(v_to_c)\n",
    "print(v_to_c.time.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 2 Day 12 & 13\n",
    "## Project: Page Visits Funnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                user_id          visit_time\n",
      "0  943647ef-3682-4750-a2e1-918ba6f16188 2017-04-07 15:14:00\n",
      "1  0c3a3dd0-fb64-4eac-bf84-ba069ce409f2 2017-01-26 14:24:00\n",
      "2  6e0b2d60-4027-4d9a-babd-0e7d40859fb1 2017-08-20 08:23:00\n",
      "3  6879527e-c5a6-4d14-b2da-50b85212b0ab 2017-11-04 18:15:00\n",
      "4  a84327ff-5daa-4ba1-b789-d5b4caf81e96 2017-02-27 11:25:00\n",
      "                                user_id           cart_time\n",
      "0  2be90e7c-9cca-44e0-bcc5-124b945ff168 2017-11-07 20:45:00\n",
      "1  4397f73f-1da3-4ab3-91af-762792e25973 2017-05-27 01:35:00\n",
      "2  a9db3d4b-0a0a-4398-a55a-ebb2c7adf663 2017-03-04 10:38:00\n",
      "3  b594862a-36c5-47d5-b818-6e9512b939b3 2017-09-27 08:22:00\n",
      "4  a68a16e2-94f0-4ce8-8ce3-784af0bbb974 2017-07-26 15:48:00\n",
      "                                user_id       checkout_time\n",
      "0  d33bdc47-4afa-45bc-b4e4-dbe948e34c0d 2017-06-25 09:29:00\n",
      "1  4ac186f0-9954-4fea-8a27-c081e428e34e 2017-04-07 20:11:00\n",
      "2  3c9c78a7-124a-4b77-8d2e-e1926e011e7d 2017-07-13 11:38:00\n",
      "3  89fe330a-8966-4756-8f7c-3bdbcd47279a 2017-04-20 16:15:00\n",
      "4  3ccdaf69-2d30-40de-b083-51372881aedd 2017-01-08 20:52:00\n",
      "                                user_id       purchase_time\n",
      "0  4b44ace4-2721-47a0-b24b-15fbfa2abf85 2017-05-11 04:25:00\n",
      "1  02e684ae-a448-408f-a9ff-dcb4a5c99aac 2017-09-05 08:45:00\n",
      "2  4b4bc391-749e-4b90-ab8f-4f6e3c84d6dc 2017-11-20 20:49:00\n",
      "3  a5dbb25f-3c36-4103-9030-9f7c6241cd8d 2017-01-22 15:18:00\n",
      "4  46a3186d-7f5a-4ab9-87af-84d05bfd4867 2017-06-11 11:32:00\n",
      "Total 2052 users visit.\n",
      "There are 1652 users don't put in their cart.\n",
      "80.50682261208577% of users don't put in their cart.\n",
      "20.930232558139537% users put in their cart but not checkout.\n",
      "                                user_id          visit_time  \\\n",
      "0  943647ef-3682-4750-a2e1-918ba6f16188 2017-04-07 15:14:00   \n",
      "1  0c3a3dd0-fb64-4eac-bf84-ba069ce409f2 2017-01-26 14:24:00   \n",
      "2  6e0b2d60-4027-4d9a-babd-0e7d40859fb1 2017-08-20 08:23:00   \n",
      "3  6e0b2d60-4027-4d9a-babd-0e7d40859fb1 2017-08-20 08:23:00   \n",
      "4  6879527e-c5a6-4d14-b2da-50b85212b0ab 2017-11-04 18:15:00   \n",
      "\n",
      "            cart_time       checkout_time       purchase_time  \n",
      "0                 NaT                 NaT                 NaT  \n",
      "1 2017-01-26 14:44:00 2017-01-26 14:54:00 2017-01-26 15:08:00  \n",
      "2 2017-08-20 08:31:00                 NaT                 NaT  \n",
      "3 2017-08-20 08:49:00                 NaT                 NaT  \n",
      "4                 NaT                 NaT                 NaT  \n",
      "14.354066985645932% checkout but not purchase\n",
      "The weakest is users don't put in their cart.\n",
      "0           NaT\n",
      "1      00:44:00\n",
      "2           NaT\n",
      "3           NaT\n",
      "4           NaT\n",
      "5           NaT\n",
      "6           NaT\n",
      "7           NaT\n",
      "8           NaT\n",
      "9           NaT\n",
      "10          NaT\n",
      "11          NaT\n",
      "12          NaT\n",
      "13          NaT\n",
      "14          NaT\n",
      "15     00:38:00\n",
      "16          NaT\n",
      "17          NaT\n",
      "18          NaT\n",
      "19          NaT\n",
      "20          NaT\n",
      "21          NaT\n",
      "22          NaT\n",
      "23          NaT\n",
      "24          NaT\n",
      "25          NaT\n",
      "26          NaT\n",
      "27          NaT\n",
      "28          NaT\n",
      "29          NaT\n",
      "         ...   \n",
      "2584        NaT\n",
      "2585        NaT\n",
      "2586        NaT\n",
      "2587        NaT\n",
      "2588        NaT\n",
      "2589        NaT\n",
      "2590        NaT\n",
      "2591        NaT\n",
      "2592        NaT\n",
      "2593   00:46:00\n",
      "2594        NaT\n",
      "2595        NaT\n",
      "2596        NaT\n",
      "2597   00:32:00\n",
      "2598   00:28:00\n",
      "2599   00:11:00\n",
      "2600   00:32:00\n",
      "2601   00:28:00\n",
      "2602   00:11:00\n",
      "2603   00:32:00\n",
      "2604   00:28:00\n",
      "2605   00:11:00\n",
      "2606        NaT\n",
      "2607        NaT\n",
      "2608        NaT\n",
      "2609        NaT\n",
      "2610        NaT\n",
      "2611        NaT\n",
      "2612        NaT\n",
      "2613        NaT\n",
      "Name: time_to_purchase, Length: 2614, dtype: timedelta64[ns]\n",
      "0 days 00:44:12.988826\n"
     ]
    }
   ],
   "source": [
    "visits = pd.read_csv('unit2_visits2.csv', parse_dates=[1])\n",
    "cart = pd.read_csv('unit2_cart2.csv', parse_dates=[1])\n",
    "checkout = pd.read_csv('unit2_checkout2.csv', parse_dates=[1])\n",
    "purchase = pd.read_csv('unit2_purchase2.csv', parse_dates=[1])\n",
    "# 1\n",
    "print(visits.head())\n",
    "print(cart.head())\n",
    "print(checkout.head())\n",
    "print(purchase.head())\n",
    "# 2\n",
    "visits_cart_left_merge = pd.merge(visits, cart,how='left')\n",
    "#print(visits_cart_left_merge.head())\n",
    "# 3\n",
    "#print(visits_cart_left_merge.user_id.count())\n",
    "#print(len(visits_cart_left_merge))\n",
    "print('Total ' + str(len(visits_cart_left_merge)) + ' users visit.')\n",
    "# 4\n",
    "null_counts_cart_time = visits_cart_left_merge[visits_cart_left_merge.cart_time.isnull()]\n",
    "#print(null_counts_cart_time.head())\n",
    "#print(null_counts_cart_time.user_id.count())\n",
    "#print(len(null_counts_cart_time))\n",
    "print('There are '+ str(len(null_counts_cart_time)) + ' users don\\'t put in their cart.')\n",
    "# 5\n",
    "not_place_in_cart = float(len(null_counts_cart_time))\n",
    "total_visits = float(len(visits_cart_left_merge))\n",
    "percentages_not_place_in_cart = not_place_in_cart / total_visits * 100\n",
    "print(str(percentages_not_place_in_cart) + '% of users don\\'t put in their cart.')\n",
    "# 6\n",
    "cart_checkout_left_merge = pd.merge(cart, checkout, how='left')\n",
    "#print(len(cart_checkout_left_merge))\n",
    "in_cart = cart_checkout_left_merge[~cart_checkout_left_merge.cart_time.isnull()]\n",
    "#not_in_cart = cart_checkout_left_merge[cart_checkout_left_merge.cart_time.isnull()]\n",
    "#print(len(in_cart))\n",
    "#print(len(not_in_cart))\n",
    "not_checkout = cart_checkout_left_merge[cart_checkout_left_merge.checkout_time.isnull()]\n",
    "#print(len(not_checkout))\n",
    "percentage_in_cart_but_not_checkout = float(len(not_checkout) / float(len(in_cart))) * 100.\n",
    "print(str(percentage_in_cart_but_not_checkout) + '% users put in their cart but not checkout.')\n",
    "# 7\n",
    "all_data = visits.merge(cart,how='left').merge(cart,how='left').merge(checkout,how='left').merge(purchase,how='left')\n",
    "print(all_data.head())\n",
    "# 8\n",
    "checkout = all_data[~all_data.checkout_time.isnull()]\n",
    "#print(len(checkout))\n",
    "checkout_but_not_purchase = all_data[(~all_data.checkout_time.isnull()) & (all_data.purchase_time.isnull())]\n",
    "#print(len(checkout_but_not_purchase))\n",
    "percentage_checkout_but_not_purchase = float(len(checkout_but_not_purchase)) / float(len(checkout)) * 100.\n",
    "print(str(percentage_checkout_but_not_purchase) + '% checkout but not purchase')\n",
    "# 9\n",
    "print('The weakest is users don\\'t put in their cart.')\n",
    "# 10\n",
    "all_data['time_to_purchase'] = all_data.purchase_time - all_data.visit_time\n",
    "# 11\n",
    "print(all_data.time_to_purchase)\n",
    "# 12\n",
    "print(all_data.time_to_purchase.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 2 Day 14\n",
    "## Project: Board Slides for FoodWheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                name neighborhood  cuisine\n",
      "0   1          Jongro BBQ      Midtown   Korean\n",
      "1   2            Pocha 32      Midtown   Korean\n",
      "2   3  Nom Wah Tea Parlor    Chinatown  Chinese\n",
      "3   4           Roberta’s     Brooklyn    Pizza\n",
      "4   5        Speedy Romeo     Brooklyn    Pizza\n",
      "      cuisine  name\n",
      "0    American    10\n",
      "1     Chinese    11\n",
      "2     Italian     8\n",
      "3    Japanese     4\n",
      "4      Korean     3\n",
      "5       Pizza     4\n",
      "6  Vegetarian     4\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "# 2\n",
    "restaurants = pd.read_csv('unit2_restaurants6.csv')\n",
    "# 3\n",
    "print(restaurants.head())\n",
    "# 4\n",
    "cuisine_options_count = restaurants.cuisine.nunique()\n",
    "#print(cuisine_options_count)\n",
    "# 5\n",
    "cuisine_counts = restaurants.groupby('cuisine').name.count().reset_index()\n",
    "print(cuisine_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
